{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "import arff\n",
    "import json\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [('id', 'NUMERIC'),\n",
    "            ('number_of_words', 'NUMERIC'),\n",
    "            ('average word length', 'NUMERIC'),\n",
    "            ('length of the longest word', 'NUMERIC'),\n",
    "            ('whether start with number', ['True', 'False']),\n",
    "            ('whether start with who/what/why/where/when/how', ['True', 'False']),\n",
    "            ('number_of_character_1_grams', 'NUMERIC'),\n",
    "            ('number_of_character_2_grams', 'NUMERIC'),\n",
    "            ('number_of_character_3_grams', 'NUMERIC'),\n",
    "            ('clindex', 'NUMERIC'),\n",
    "            ('formality_measure', 'NUMERIC'),\n",
    "            ('is_exclamation_question_mark_present', ['0', '1']),\n",
    "            ('lix', 'NUMERIC'),\n",
    "            ('number_of_uppercase_words', 'NUMERIC'),\n",
    "            ('rix', 'NUMERIC'),\n",
    "            ('number_of_word_1_grams', 'NUMERIC'),\n",
    "#             ('number_of_contractions','NUMERIC'),\n",
    "            ('label', ['0', '1']),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/10677020/real-word-count-in-nltk\n",
    "def number_of_words(text):\n",
    "# TODO\n",
    "#     regexptokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     words = regexptokenizer.tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "\n",
    "def number_of_character_1_grams(text):\n",
    "    characters = [c for c in text]\n",
    "    onegrams = ngrams(characters, 1)\n",
    "    return len([gram for gram in onegrams])\n",
    "\n",
    "\n",
    "def number_of_character_2_grams(text):\n",
    "    if len(text) == 0:\n",
    "        return []\n",
    "    characters = [c for c in text]\n",
    "    twograms = ngrams(characters, 2)\n",
    "    return len([gram for gram in twograms])\n",
    "\n",
    "\n",
    "def number_of_character_3_grams(text):\n",
    "    if len(text) <= 1:\n",
    "        return 0\n",
    "    characters = [c for c in text]\n",
    "    threegrams = ngrams(characters, 3)\n",
    "    return len([gram for gram in threegrams])\n",
    "\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index\n",
    "def clindex(text):\n",
    "    text_lower = text.lower()\n",
    "    number_of_letters = 0\n",
    "    for character in text_lower:\n",
    "        if character in ascii_lowercase:\n",
    "            number_of_letters += 1\n",
    "    number_of_sentences = len(sent_tokenize(text))\n",
    "    n_of_words = number_of_words(text)\n",
    "    l = 0\n",
    "    s = 0\n",
    "    # TODO should l and s be 0?\n",
    "    if n_of_words == 0:\n",
    "        pass\n",
    "    else:\n",
    "        # l = Letters ÷ Words × 100\n",
    "        l = number_of_letters / n_of_words * 100\n",
    "        # s = Sentences ÷ Words × 100\n",
    "        s = number_of_sentences / n_of_words * 100\n",
    "    return 0.0588 * l - 0.296 * s - 15.8\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/10674832/count-verbs-nouns-and-other-parts-of-speech-with-pythons-nltk\n",
    "def formality_measure(text):\n",
    "    tokenized_text = nltk.word_tokenize(text.lower())\n",
    "    t = nltk.Text(tokenized_text)\n",
    "    pos_tags = nltk.pos_tag(t)\n",
    "    counts = Counter(tag for word,tag in pos_tags)\n",
    "    return (counts['NN'] + counts['NNP'] + counts['NNS'] + counts['JJ'] + counts['JJR'] + counts['JJS'] + counts['IN'] + counts['DT'] + counts['PDT'] + counts['WDT'] - counts['PRP'] - counts['PRP$'] - counts['WP'] - counts['WP$'] - counts['VB'] - counts['VBD'] - counts['VBG'] - counts['VBN'] - counts['VBP'] - counts['VBZ'] - counts['RB'] - counts['RBR'] - counts['RBS'] - counts['WRB'] - counts['UH'] + 100) / 2\n",
    "\n",
    "\n",
    "def is_exclamation_question_mark_present(text):\n",
    "    return 0 if '!' not in text and '?' not in text else 1\n",
    "\n",
    "\n",
    "def lix(text):\n",
    "    # TODO should we return 0?\n",
    "    if len(sent_tokenize(text)) == 0:\n",
    "        return 0\n",
    "    return number_of_words(text) / len(sent_tokenize(text))\n",
    "\n",
    "\n",
    "def number_of_uppercase_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    n_of_uppercase_words = 0\n",
    "    for word in words:\n",
    "        if word[0] in ascii_uppercase:\n",
    "            n_of_uppercase_words += 1\n",
    "    return n_of_uppercase_words\n",
    "\n",
    "def rix(text):\n",
    "    lw = 0\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        if len(word) >= 7:\n",
    "            lw += 1\n",
    "    # TODO should we return 0?\n",
    "    if len(sent_tokenize(text)) == 0:\n",
    "        return 0\n",
    "    return lw / len(sent_tokenize(text))\n",
    "\n",
    "\n",
    "def number_of_word_1_grams(text):\n",
    "    onegrams = ngrams(word_tokenize(text), 1)\n",
    "    return len([gram for gram in onegrams])\n",
    "\n",
    "\n",
    "# def number_of_contractions(text):\n",
    "#     stripped_contractions = ['aint', 'amnt', 'arent', 'cant', 'couldve', 'couldnt', 'couldntve',\n",
    "#                 'didnt', 'doesnt', 'dont', 'gonna', 'gotta', 'hadnt', 'hadntve', 'hasnt',\n",
    "#                 'havent','hell', 'hes', 'hesnt', 'howd', 'howll',\n",
    "#                 'hows', 'id', 'idnt', 'idve', 'ill', 'im', 'ive', 'ivent', 'isnt',\n",
    "#                 'itd', 'itll', 'its', 'itsnt', 'mightnt','mightve', 'mustnt', 'mustntve', 'mustve', 'neednt', 'ol', 'oughtnt',\n",
    "#                 'shant', 'shed', 'shes', 'shouldve','shouldnt', 'shouldntve', 'somebodydve', 'somebodydntve', 'somebodys',\n",
    "#                 'someonell', 'someones','somethingd', 'somethingdnt', 'somethingdntve', 'somethingdve', 'somethingll',\n",
    "#                 'somethings', 'thatll', 'thats', 'thatd', 'thered', 'therednt', 'theredntve',\n",
    "#                 'theredve', 'therere', 'theres', 'theyd', 'theydnt', 'theydntve', 'theydve',\n",
    "#                 'theyll', 'theyontve', 'theyre', 'theyve', 'theyvent', 'wasnt',\n",
    "#                 'wed', 'wedve', 'wednt', 'wedntve', 'well', 'wontve', 'were', 'weve', 'werent',\n",
    "#                 'whatd', 'whatll', 'whats', 'whatve', 'whens', 'whered', 'wheres',\n",
    "#                 'whereve', 'whod', 'whodve', 'wholl', 'whore', 'whos', 'whove', 'whyd', 'whyre',\n",
    "#                 'whys', 'wont', 'wontve', 'wouldve', 'wouldnt', 'wouldntve', 'yall', 'yallllve', 'yallre', 'yallllvent', 'yaint',\n",
    "#                 'youd', 'youdve', 'youll', 'youre', 'yourent', 'youve', 'youvent']\n",
    "#     words = word_tokenize(text)\n",
    "#     num = len([word.replace(\"'\",\"\") for word in words if word in stripped_contractions])\n",
    "#     return(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet_id, text):\n",
    "    doc = text.strip().split(' ')\n",
    "    f1 = 0\n",
    "    f2 = 0\n",
    "    f3 = 0\n",
    "    f4 = False\n",
    "    f5 = False\n",
    "    for token in doc:\n",
    "        word = token.lower()\n",
    "        if f1 == 0:\n",
    "            if word[0].isdigit():\n",
    "                f4 = True\n",
    "            if word in ['who', 'what', 'why', 'where', 'when', 'how']:\n",
    "                f5 = True\n",
    "        f1 += 1\n",
    "        length = len(word)\n",
    "        f2 += length\n",
    "        f3 = max(f3, length)\n",
    "    if f1 == 0:\n",
    "        return False\n",
    "    return (tweet_id, f1, f2 * 1.0 / f1, f3, f4, f5, number_of_character_1_grams(text), number_of_character_2_grams(text), number_of_character_3_grams(text), clindex(text), formality_measure(text), is_exclamation_question_mark_present(text), lix(text), number_of_uppercase_words(text), rix(text), number_of_word_1_grams(text), number_of_contractions(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ipython/ipython/issues/10123\n",
    "directory_path = os.getcwd()\n",
    "dataset_no_figures_path = directory_path + '/../data/dataset_no_figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17535\n"
     ]
    }
   ],
   "source": [
    "id_features = {}\n",
    "with open(dataset_no_figures_path + 'instances_train.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        dic = json.loads(line)\n",
    "        if len(dic['postText'][0]) > 0:\n",
    "            feat = extract_features(dic['id'], dic['postText'][0])\n",
    "            if feat:\n",
    "                id_features.setdefault(dic['id'], feat)\n",
    "print(len(id_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17535\n"
     ]
    }
   ],
   "source": [
    "id_labels = {}\n",
    "with open(dataset_no_figures_path + 'truth_train.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        dic = json.loads(line)\n",
    "        label = 1\n",
    "        if dic['truthClass'][0] == 'n':\n",
    "            label = 0\n",
    "        if dic['id'] in id_features:\n",
    "            id_labels.setdefault(dic['id'], label)\n",
    "print(len(id_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data.setdefault('attributes', features)\n",
    "data.setdefault('description', '')\n",
    "data.setdefault('relation', 'team-one')\n",
    "data.setdefault('data', [])\n",
    "for i in id_labels:\n",
    "    tmp = [_ for _ in id_features[i]]\n",
    "    tmp.append(str(id_labels[i]))\n",
    "    data['data'].append(tmp)\n",
    "\n",
    "with open(dataset_no_figures_path + 'sample_train.arff', 'w') as f:\n",
    "    f.write(arff.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate test .arff file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4408\n"
     ]
    }
   ],
   "source": [
    "id_features = {}\n",
    "id_labels = {}\n",
    "with open(dataset_no_figures_path + 'instances_test.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        dic = json.loads(line)\n",
    "        if len(dic['postText'][0]) > 0:\n",
    "            feat = extract_features(dic['id'], dic['postText'][0])\n",
    "            if feat:\n",
    "                id_features.setdefault(dic['id'], feat)\n",
    "            if dic['id'] in id_features:\n",
    "                id_labels.setdefault(dic['id'], '?')\n",
    "print(len(id_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data.setdefault('attributes', features)\n",
    "data.setdefault('description', '')\n",
    "data.setdefault('relation', 'team-one')\n",
    "data.setdefault('data', [])\n",
    "for i in id_labels:\n",
    "    tmp = [_ for _ in id_features[i]]\n",
    "    tmp.append(str(id_labels[i]))\n",
    "    data['data'].append(tmp)\n",
    "\n",
    "with open(dataset_no_figures_path + 'sample_test.arff', 'w') as f:\n",
    "    f.write(arff.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
