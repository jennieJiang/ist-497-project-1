{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# https://github.com/ipython/ipython/issues/10123\n",
    "directory_path = os.getcwd()\n",
    "dataset_no_figures_path = directory_path + '/../data/dataset_no_figures/'\n",
    "\n",
    "is_clickbait = {}\n",
    "\n",
    "with open(dataset_no_figures_path + 'truth_train.jsonl') as f:\n",
    "    for line in f:\n",
    "        truth = json.loads(line)\n",
    "        is_clickbait[truth['id']] = 0 if truth['truthClass'] == 'no-clickbait' else 1\n",
    "        \n",
    "df = pd.DataFrame()\n",
    "\n",
    "with open(dataset_no_figures_path + 'instances_train.jsonl') as f:\n",
    "    for line in f:\n",
    "        instance = json.loads(line)\n",
    "        data = pd.DataFrame({'post_text': instance['postText'], 'is_clickbait': is_clickbait[instance['id']]}, index=[instance['id']])\n",
    "        df = df.append(data)\n",
    "        \n",
    "# print(df)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data\n",
    "\n",
    "TODO remove newlines from postText? (e.g., \\n in 17560)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "* Coleman-Liau score (CLScore)\n",
    "* RIX and LIX indices\n",
    "* Formality measure (fmeasure)\n",
    "* Number of uppercase words, presence of questionmarks and exclamation marks in headlines (titles), and the length of the title (number of words) are the most important content features\n",
    "\n",
    "\n",
    "* The character n-gram features and the word 1-gram feature appear to contribute most to performance\n",
    "    * Character n-grams are known to capture writing style\n",
    "\n",
    "\n",
    "* headline: word count\n",
    "* body: 1. Informality: We compute the frequencies of two informality indicators, namely internet slang and bait words. Additionally, the length of news bodies is also an input feature.\n",
    "\n",
    "\n",
    "* Sent length, word length, ratio of stop words to content words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "# TODO\n",
    "# from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "import nltk\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/10677020/real-word-count-in-nltk\n",
    "def number_of_words(text):\n",
    "# TODO\n",
    "#     regexptokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     words = regexptokenizer.tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "\n",
    "def number_of_character_1_grams(text):\n",
    "    characters = [c for c in text]\n",
    "    onegrams = ngrams(characters, 1)\n",
    "    return len([gram for gram in onegrams])\n",
    "\n",
    "\n",
    "def number_of_character_2_grams(text):\n",
    "    if len(text) == 0:\n",
    "        return []\n",
    "    characters = [c for c in text]\n",
    "    twograms = ngrams(characters, 2)\n",
    "    return len([gram for gram in twograms])\n",
    "\n",
    "\n",
    "def number_of_character_3_grams(text):\n",
    "    if len(text) <= 1:\n",
    "        return 0\n",
    "    characters = [c for c in text]\n",
    "    threegrams = ngrams(characters, 3)\n",
    "    return len([gram for gram in threegrams])\n",
    "\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index\n",
    "def clindex(text):\n",
    "    text_lower = text.lower()\n",
    "    number_of_letters = 0\n",
    "    for character in text_lower:\n",
    "        if character in ascii_lowercase:\n",
    "            number_of_letters += 1\n",
    "    number_of_sentences = len(sent_tokenize(text))\n",
    "    n_of_words = number_of_words(text)\n",
    "    l = 0\n",
    "    s = 0\n",
    "    # TODO should l and s be 0?\n",
    "    if n_of_words == 0:\n",
    "        pass\n",
    "    else:\n",
    "        # l = Letters ÷ Words × 100\n",
    "        l = number_of_letters / n_of_words * 100\n",
    "        # s = Sentences ÷ Words × 100\n",
    "        s = number_of_sentences / n_of_words * 100\n",
    "    return 0.0588 * l - 0.296 * s - 15.8\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/10674832/count-verbs-nouns-and-other-parts-of-speech-with-pythons-nltk\n",
    "def formality_measure(text):\n",
    "    tokenized_text = nltk.word_tokenize(text.lower())\n",
    "    t = nltk.Text(tokenized_text)\n",
    "    pos_tags = nltk.pos_tag(t)\n",
    "    counts = Counter(tag for word,tag in pos_tags)\n",
    "    return (counts['NN'] + counts['NNP'] + counts['NNS'] + counts['JJ'] + counts['JJR'] + counts['JJS'] + counts['IN'] + counts['DT'] + counts['PDT'] + counts['WDT'] - counts['PRP'] - counts['PRP$'] - counts['WP'] - counts['WP$'] - counts['VB'] - counts['VBD'] - counts['VBG'] - counts['VBN'] - counts['VBP'] - counts['VBZ'] - counts['RB'] - counts['RBR'] - counts['RBS'] - counts['WRB'] - counts['UH'] + 100) / 2\n",
    "\n",
    "\n",
    "def is_exclamation_question_mark_present(text):\n",
    "    return 0 if '!' not in text and '?' not in text else 1\n",
    "\n",
    "\n",
    "def lix(text):\n",
    "    # TODO should we return 0?\n",
    "    if len(sent_tokenize(text)) == 0:\n",
    "        return 0\n",
    "    return number_of_words(text) / len(sent_tokenize(text))\n",
    "\n",
    "\n",
    "def number_of_uppercase_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    n_of_uppercase_words = 0\n",
    "    for word in words:\n",
    "        if word[0] in ascii_uppercase:\n",
    "            n_of_uppercase_words += 1\n",
    "    return n_of_uppercase_words\n",
    "\n",
    "\n",
    "def rix(text):\n",
    "    lw = 0\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        if len(word) >= 7:\n",
    "            lw += 1\n",
    "    # TODO should we return 0?\n",
    "    if len(sent_tokenize(text)) == 0:\n",
    "        return 0\n",
    "    return lw / len(sent_tokenize(text))\n",
    "\n",
    "\n",
    "def number_of_word_1_grams(text):\n",
    "    onegrams = ngrams(word_tokenize(text), 1)\n",
    "    return len([gram for gram in onegrams])\n",
    "\n",
    "\n",
    "df['number_of_character_1_grams'] = None\n",
    "df['number_of_character_2_grams'] = None\n",
    "df['number_of_character_3_grams'] = None\n",
    "df['clindex'] = None\n",
    "df['formality_measure'] = None\n",
    "df['is_exclamation_question_mark_present'] = None\n",
    "df['lix'] = None\n",
    "df['number_of_uppercase_words'] = None\n",
    "df['number_of_words'] = None\n",
    "df['rix'] = None\n",
    "df['number_of_word_1_grams'] = None\n",
    "for i in df.index:\n",
    "    df.at[i, 'number_of_character_1_grams'] = number_of_character_1_grams(df.loc[i]['post_text'])\n",
    "    df.at[i, 'number_of_character_2_grams'] = number_of_character_2_grams(df.loc[i]['post_text'])\n",
    "    df.at[i, 'number_of_character_3_grams'] = number_of_character_3_grams(df.loc[i]['post_text'])\n",
    "    df.at[i, 'clindex'] = clindex(df.loc[i]['post_text'])\n",
    "    df.at[i, 'formality_measure'] = formality_measure(df.loc[i]['post_text'])\n",
    "    df.at[i, 'is_exclamation_question_mark_present'] = is_exclamation_question_mark_present(df.loc[i]['post_text'])\n",
    "    df.at[i, 'lix'] = lix(df.loc[i]['post_text'])\n",
    "    df.at[i, 'number_of_uppercase_words'] = number_of_uppercase_words(df.loc[i]['post_text'])\n",
    "    df.at[i, 'number_of_words'] = number_of_words(df.loc[i]['post_text'])\n",
    "    df.at[i, 'rix'] = rix(df.loc[i]['post_text'])\n",
    "    df.at[i, 'number_of_word_1_grams'] = number_of_word_1_grams(df.loc[i]['post_text'])\n",
    "# print(df)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize features\n",
    "\n",
    "* TODO get features in range of [0,1] or [-1,1]?\n",
    "* TODO convert lix to five levels (0-4) based on this: very easy (0-24), easy (25-34), standard (35-44), difficult (45-54) and very difficult (more than 55)\n",
    "* TODO convert rix to thirteen levels (0-13) based on this: 0.2, 0.5, 0.8, 1.3, 1.8, 2.4, 3.0, 3.7, 4.5, 5.3, 6.2, 7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(df):\n",
    "    test_ratio = 0.2\n",
    "\n",
    "    test_set_size = int(len(df) * test_ratio)\n",
    "\n",
    "    train_set = df[:len(df) - test_set_size]\n",
    "    test_set = df[len(df) - test_set_size:]\n",
    "\n",
    "#     print('train_set length:', len(train_set))\n",
    "#     print('test_set length:', len(test_set))\n",
    "\n",
    "    test_set = test_set.drop('is_clickbait', 1)\n",
    "    \n",
    "    return train_set, test_set\n",
    "\n",
    "train_set, test_set = create_train_test_sets(df)\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# TODO try logistic regression, naive bayes, random forest\n",
    "\n",
    "MODEL_SCORES_FILENAME_PREPEND = 'model_scores_'\n",
    "\n",
    "failures_dict = {}\n",
    "\n",
    "\n",
    "def mae(y_test, predictions):\n",
    "    lin_mae = mean_absolute_error(y_test, predictions)\n",
    "    return lin_mae\n",
    "\n",
    "\n",
    "def mse(y_test, predictions):\n",
    "    lin_mse = mean_squared_error(y_test, predictions)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    return lin_rmse\n",
    "\n",
    "\n",
    "def score_models(predictors_prepared, labels, X_test_prepared, y_test, max_features_one, max_features_two):\n",
    "    print('scoring models')\n",
    "    \n",
    "    names = []\n",
    "    mses = []\n",
    "    maes = []\n",
    "    \n",
    "    if 'linear_regression' not in failures_dict:\n",
    "        try:\n",
    "            linear_regression = LinearRegression()\n",
    "            linear_regression.fit(predictors_prepared, labels)\n",
    "            predictions = linear_regression.predict(X_test_prepared)\n",
    "            mse_value = mse(y_test, predictions)\n",
    "            mae_value = mae(y_test, predictions)\n",
    "            mses.append(mse_value)\n",
    "            maes.append(mae_value)\n",
    "            names.append('linear_regression')\n",
    "            print('finished linear_regression')\n",
    "        except:\n",
    "            print('linear_regression failed')\n",
    "            failures_dict['linear_regression'] = 1\n",
    "    else:\n",
    "        print('SKIPPING linear_regression, failed before')\n",
    "        \n",
    "#     if 'svr_linear' not in failures_dict:\n",
    "#         try:\n",
    "#             svr_linear = SVR(kernel='linear', C=1e3)\n",
    "#             svr_linear.fit(predictors_prepared, labels)\n",
    "#             predictions = svr_linear.predict(X_test_prepared)\n",
    "#             mse_value = mse(y_test, predictions)\n",
    "#             mae_value = mae(y_test, predictions)\n",
    "#             mses.append(mse_value)\n",
    "#             maes.append(mae_value)\n",
    "#             names.append('svr_linear')\n",
    "#             print('finished svr_linear')\n",
    "#         except:\n",
    "#             print('svr_linear failed')\n",
    "#             failures_dict['svr_linear'] = 1\n",
    "#     else:\n",
    "#         print('SKIPPING svr_linear, failed before')\n",
    "    \n",
    "#     if 'svr_polynomial' not in failures_dict:\n",
    "#         try:\n",
    "#             svr_polynomial = SVR(kernel='poly', C=1e3, degree=2)\n",
    "#             svr_polynomial.fit(predictors_prepared, labels)\n",
    "#             predictions = svr_polynomial.predict(X_test_prepared)\n",
    "#             mse_value = mse(y_test, predictions)\n",
    "#             mae_value = mae(y_test, predictions)\n",
    "#             mses.append(mse_value)\n",
    "#             maes.append(mae_value)\n",
    "#             names.append('svr_polynomial')\n",
    "#             print('finished svr_polynomial')\n",
    "#         except:\n",
    "#             print('svr_polynomial failed')\n",
    "#             failures_dict['svr_polynomial'] = 1\n",
    "#     else:\n",
    "#         print('SKIPPING svr_polynomial, failed before')\n",
    "\n",
    "    if 'svr_rbf' not in failures_dict:\n",
    "        try:\n",
    "            svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "            svr_rbf.fit(predictors_prepared, labels)\n",
    "            predictions = svr_rbf.predict(X_test_prepared)\n",
    "            mse_value = mse(y_test, predictions)\n",
    "            mae_value = mae(y_test, predictions)\n",
    "            mses.append(mse_value)\n",
    "            maes.append(mae_value)\n",
    "            names.append('svr_rbf')\n",
    "            print('finished svr_rbf')\n",
    "        except:\n",
    "            print('svr_rbf failed')\n",
    "            failures_dict['svr_rbf'] = 1\n",
    "    else:\n",
    "        print('SKIPPING svr_rbf, failed before')\n",
    "    \n",
    "    if 'svr' not in failures_dict:\n",
    "        try:\n",
    "            svr = SVR()\n",
    "            svr.fit(predictors_prepared, labels)\n",
    "            predictions = svr.predict(X_test_prepared)\n",
    "            mse_value = mse(y_test, predictions)\n",
    "            mae_value = mae(y_test, predictions)\n",
    "            mses.append(mse_value)\n",
    "            maes.append(mae_value)\n",
    "            names.append('svr')\n",
    "            print('finished svr')\n",
    "        except:\n",
    "            print('svr failed')\n",
    "            failures_dict['svr'] = 1\n",
    "    else:\n",
    "        print('SKIPPING svr, failed before')\n",
    "    \n",
    "    if 'ridge' not in failures_dict:\n",
    "        try:\n",
    "            ridge = Ridge(alpha=.5)\n",
    "            ridge.fit(predictors_prepared, labels)\n",
    "            predictions = ridge.predict(X_test_prepared)\n",
    "            mse_value = mse(y_test, predictions)\n",
    "            mae_value = mae(y_test, predictions)\n",
    "            mses.append(mse_value)\n",
    "            maes.append(mae_value)\n",
    "            names.append('ridge')\n",
    "            print('finished ridge')\n",
    "        except:\n",
    "            print('ridge failed')\n",
    "            failures_dict['ridge'] = 1\n",
    "    else:\n",
    "        print('SKIPPING ridge, failed before')\n",
    "    \n",
    "    if 'ridge_cv' not in failures_dict:\n",
    "        try:\n",
    "            ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "            ridge_cv.fit(predictors_prepared, labels)\n",
    "            predictions = ridge_cv.predict(X_test_prepared)\n",
    "            mse_value = mse(y_test, predictions)\n",
    "            mae_value = mae(y_test, predictions)\n",
    "            mses.append(mse_value)\n",
    "            maes.append(mae_value)\n",
    "            names.append('ridge_cv')\n",
    "            print('finished ridge_cv')\n",
    "        except:\n",
    "            print('ridge_cv failed')\n",
    "            failures_dict['ridge_cv'] = 1\n",
    "    else:\n",
    "        print('SKIPPING ridge_cv, failed before')\n",
    "    \n",
    "    if 'lasso' not in failures_dict:\n",
    "        try:\n",
    "            lasso = Lasso(alpha=0.1)\n",
    "            lasso.fit(predictors_prepared, labels)\n",
    "            predictions = lasso.predict(X_test_prepared)\n",
    "            mse_value = mse(y_test, predictions)\n",
    "            mae_value = mae(y_test, predictions)\n",
    "            mses.append(mse_value)\n",
    "            maes.append(mae_value)\n",
    "            names.append('lasso')\n",
    "            print('finished lasso')\n",
    "        except:\n",
    "            print('lasso failed')\n",
    "            failures_dict['lasso'] = 1\n",
    "    else:\n",
    "        print('SKIPPING lasso, failed before')\n",
    "    \n",
    "    if 'bayesian_ridge' not in failures_dict:\n",
    "        try:\n",
    "            bayesian_ridge = BayesianRidge()\n",
    "            bayesian_ridge.fit(predictors_prepared, labels)\n",
    "            predictions = bayesian_ridge.predict(X_test_prepared)\n",
    "            mse_value = mse(y_test, predictions)\n",
    "            mae_value = mae(y_test, predictions)\n",
    "            mses.append(mse_value)\n",
    "            maes.append(mae_value)\n",
    "            names.append('bayesian_ridge')\n",
    "            print('finished bayesian_ridge')\n",
    "        except:\n",
    "            print('bayesian_ridge failed')\n",
    "            failures_dict['bayesian_ridge'] = 1\n",
    "    else:\n",
    "        print('SKIPPING bayesian_ridge, failed before')\n",
    "    \n",
    "    if 'perceptron' not in failures_dict:\n",
    "        try:\n",
    "            perceptron = Perceptron()\n",
    "            perceptron.fit(predictors_prepared, labels)\n",
    "            predictions = perceptron.predict(X_test_prepared)\n",
    "            mse_value = mse(y_test, predictions)\n",
    "            mae_value = mae(y_test, predictions)\n",
    "            mses.append(mse_value)\n",
    "            maes.append(mae_value)\n",
    "            names.append('perceptron')\n",
    "            print('finished perceptron')\n",
    "        except:\n",
    "            print('perceptron failed')\n",
    "            failures_dict['perceptron'] = 1\n",
    "    else:\n",
    "        print('SKIPPING perceptron, failed before')\n",
    "    \n",
    "    if 'lasso_lars' not in failures_dict:\n",
    "        try:\n",
    "            lasso_lars = LassoLars(alpha=.1)\n",
    "            lasso_lars.fit(predictors_prepared, labels)\n",
    "            predictions = lasso_lars.predict(X_test_prepared)\n",
    "            mse_value = mse(y_test, predictions)\n",
    "            mae_value = mae(y_test, predictions)\n",
    "            mses.append(mse_value)\n",
    "            maes.append(mae_value)\n",
    "            names.append('lasso_lars')\n",
    "            print('finished lasso_lars')\n",
    "        except:\n",
    "            print('lasso_lars failed')\n",
    "            failures_dict['lasso_lars'] = 1\n",
    "    else:\n",
    "        print('SKIPPING lasso_lars, failed before')\n",
    "    \n",
    "    if 'grid_search_cv' not in failures_dict:\n",
    "        try:\n",
    "            param_grid = [\n",
    "                # try 12 (3×4) combinations of hyperparameters\n",
    "                {'n_estimators': [3, 10, 30], 'max_features': max_features_one},\n",
    "                # then try 6 (2×3) combinations with bootstrap set as False\n",
    "                {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': max_features_two},\n",
    "            ]\n",
    "            forest_reg = RandomForestRegressor()\n",
    "            # train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "            grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "            grid_search.fit(predictors_prepared, labels)\n",
    "            final_model = grid_search.best_estimator_\n",
    "            predictions = final_model.predict(X_test_prepared)\n",
    "            mse_value = mse(y_test, predictions)\n",
    "            mae_value = mae(y_test, predictions)\n",
    "            mses.append(mse_value)\n",
    "            maes.append(mae_value)\n",
    "            names.append('grid_search_cv')\n",
    "            print('finished grid_search_cv')\n",
    "        except:\n",
    "            print('grid_search_cv failed')\n",
    "            failures_dict['grid_search_cv'] = 1\n",
    "    else:\n",
    "        print('SKIPPING grid_search_cv, failed before')\n",
    "    \n",
    "    with open(MODEL_SCORES_FILENAME_PREPEND + '.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(['name', 'mse', 'mae'])\n",
    "\n",
    "        for i in range(0, len(names)):\n",
    "            writer.writerow([names[i], mses[i], maes[i]])\n",
    "    \n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrameSelector\n",
    "\n",
    "Scikit-Learn doesn't handle DataFrames yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def get_pipeline(predictors):\n",
    "    attributes = list(predictors)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "            ('selector', DataFrameSelector(attributes)),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print('starting')\n",
    "\n",
    "MODEL_FAILURES_FILENAME = 'model_failures.csv'\n",
    "\n",
    "# if os.path.exists(MODEL_SCORES_FILENAME):\n",
    "#     os.remove(MODEL_SCORES_FILENAME)\n",
    "    \n",
    "if os.path.exists(MODEL_FAILURES_FILENAME):\n",
    "    os.remove(MODEL_FAILURES_FILENAME)\n",
    "\n",
    "print('deleted?')\n",
    "    \n",
    "# separate predictors and labels\n",
    "predictors = train_set.drop('is_clickbait', axis=1) # drop labels for training set\n",
    "labels = train_set.loc[:, 'is_clickbait']\n",
    "\n",
    "print('separated predictors and labels')\n",
    "\n",
    "# prepare predictors\n",
    "pipeline = get_pipeline(predictors)\n",
    "print('got pipeline')\n",
    "predictors_prepared = pipeline.fit_transform(predictors)\n",
    "\n",
    "print('done')\n",
    "\n",
    "# print('prepared predictors')\n",
    "\n",
    "# # prepare X_test  \n",
    "# X_test_prepared = pipeline.fit_transform(X_test)\n",
    "# y_test = test_set.loc[:, 'is_clickbait']\n",
    "\n",
    "# print('prepared X_test')\n",
    "\n",
    "# max_features_one = [2, 4, 6]\n",
    "# max_features_two = [2, 3, 4]\n",
    "    \n",
    "# score_models(predictors_prepared, labels, X_test_prepared, y_test, max_features_one, max_features_two)\n",
    "    \n",
    "# with open(MODEL_FAILURES_FILENAME, 'w+', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile, delimiter=',')\n",
    "\n",
    "#         for key in failures_dict.items():\n",
    "#             writer.writerow([key])\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create error functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
