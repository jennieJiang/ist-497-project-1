{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# https://github.com/ipython/ipython/issues/10123\n",
    "directory_path = os.getcwd()\n",
    "dataset_no_figures_path = directory_path + '/../data/dataset_no_figures/'\n",
    "\n",
    "is_clickbait = {}\n",
    "\n",
    "with open(dataset_no_figures_path + 'truth_train.jsonl') as f:\n",
    "    for line in f:\n",
    "        truth = json.loads(line)\n",
    "        is_clickbait[truth['id']] = 0 if truth['truthClass'] == 'no-clickbait' else 1\n",
    "        \n",
    "df = pd.DataFrame()\n",
    "\n",
    "with open(dataset_no_figures_path + 'instances_train.jsonl') as f:\n",
    "    for line in f:\n",
    "        instance = json.loads(line)\n",
    "        data = pd.DataFrame({'post_text': instance['postText'], 'is_clickbait': is_clickbait[instance['id']]}, index=[instance['id']])\n",
    "        df = df.append(data)\n",
    "        \n",
    "# print(df)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data\n",
    "\n",
    "TODO remove newlines from postText? (e.g., \\n in 17560)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "* Coleman-Liau score (CLScore)\n",
    "* RIX and LIX indices\n",
    "* Formality measure (fmeasure)\n",
    "* Number of uppercase words, presence of questionmarks and exclamation marks in headlines (titles), and the length of the title (number of words) are the most important content features\n",
    "\n",
    "\n",
    "* The character n-gram features and the word 1-gram feature appear to contribute most to performance\n",
    "    * Character n-grams are known to capture writing style\n",
    "\n",
    "\n",
    "* headline: word count\n",
    "* body: 1. Informality: We compute the frequencies of two informality indicators, namely internet slang and bait words. Additionally, the length of news bodies is also an input feature.\n",
    "\n",
    "\n",
    "* Sent length, word length, ratio of stop words to content words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "# TODO\n",
    "# from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "import nltk\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/10677020/real-word-count-in-nltk\n",
    "def number_of_words(text):\n",
    "# TODO\n",
    "#     regexptokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     words = regexptokenizer.tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "\n",
    "def number_of_character_1_grams(text):\n",
    "    characters = [c for c in text]\n",
    "    onegrams = ngrams(characters, 1)\n",
    "    return len([gram for gram in onegrams])\n",
    "\n",
    "\n",
    "def number_of_character_2_grams(text):\n",
    "    if len(text) == 0:\n",
    "        return []\n",
    "    characters = [c for c in text]\n",
    "    twograms = ngrams(characters, 2)\n",
    "    return len([gram for gram in twograms])\n",
    "\n",
    "\n",
    "def number_of_character_3_grams(text):\n",
    "    if len(text) <= 1:\n",
    "        return 0\n",
    "    characters = [c for c in text]\n",
    "    threegrams = ngrams(characters, 3)\n",
    "    return len([gram for gram in threegrams])\n",
    "\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index\n",
    "def clindex(text):\n",
    "    text_lower = text.lower()\n",
    "    number_of_letters = 0\n",
    "    for character in text_lower:\n",
    "        if character in ascii_lowercase:\n",
    "            number_of_letters += 1\n",
    "    number_of_sentences = len(sent_tokenize(text))\n",
    "    n_of_words = number_of_words(text)\n",
    "    l = 0\n",
    "    s = 0\n",
    "    # TODO should l and s be 0?\n",
    "    if n_of_words == 0:\n",
    "        pass\n",
    "    else:\n",
    "        # l = Letters ÷ Words × 100\n",
    "        l = number_of_letters / n_of_words * 100\n",
    "        # s = Sentences ÷ Words × 100\n",
    "        s = number_of_sentences / n_of_words * 100\n",
    "    return 0.0588 * l - 0.296 * s - 15.8\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/10674832/count-verbs-nouns-and-other-parts-of-speech-with-pythons-nltk\n",
    "def formality_measure(text):\n",
    "    tokenized_text = nltk.word_tokenize(text.lower())\n",
    "    t = nltk.Text(tokenized_text)\n",
    "    pos_tags = nltk.pos_tag(t)\n",
    "    counts = Counter(tag for word,tag in pos_tags)\n",
    "    return (counts['NN'] + counts['NNP'] + counts['NNS'] + counts['JJ'] + counts['JJR'] + counts['JJS'] + counts['IN'] + counts['DT'] + counts['PDT'] + counts['WDT'] - counts['PRP'] - counts['PRP$'] - counts['WP'] - counts['WP$'] - counts['VB'] - counts['VBD'] - counts['VBG'] - counts['VBN'] - counts['VBP'] - counts['VBZ'] - counts['RB'] - counts['RBR'] - counts['RBS'] - counts['WRB'] - counts['UH'] + 100) / 2\n",
    "\n",
    "\n",
    "def is_exclamation_question_mark_present(text):\n",
    "    return 0 if '!' not in text and '?' not in text else 1\n",
    "\n",
    "\n",
    "def lix(text):\n",
    "    # TODO should we return 0?\n",
    "    if len(sent_tokenize(text)) == 0:\n",
    "        return 0\n",
    "    return number_of_words(text) / len(sent_tokenize(text))\n",
    "\n",
    "\n",
    "def number_of_uppercase_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    n_of_uppercase_words = 0\n",
    "    for word in words:\n",
    "        if word[0] in ascii_uppercase:\n",
    "            n_of_uppercase_words += 1\n",
    "    return n_of_uppercase_words\n",
    "\n",
    "\n",
    "def rix(text):\n",
    "    lw = 0\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        if len(word) >= 7:\n",
    "            lw += 1\n",
    "    # TODO should we return 0?\n",
    "    if len(sent_tokenize(text)) == 0:\n",
    "        return 0\n",
    "    return lw / len(sent_tokenize(text))\n",
    "\n",
    "\n",
    "def number_of_word_1_grams(text):\n",
    "    onegrams = ngrams(word_tokenize(text), 1)\n",
    "    return len([gram for gram in onegrams])\n",
    "\n",
    "\n",
    "df['number_of_character_1_grams'] = None\n",
    "df['number_of_character_2_grams'] = None\n",
    "df['number_of_character_3_grams'] = None\n",
    "df['clindex'] = None\n",
    "df['formality_measure'] = None\n",
    "df['is_exclamation_question_mark_present'] = None\n",
    "df['lix'] = None\n",
    "df['number_of_uppercase_words'] = None\n",
    "df['number_of_words'] = None\n",
    "df['rix'] = None\n",
    "df['number_of_word_1_grams'] = None\n",
    "for i in df.index:\n",
    "    df.at[i, 'number_of_character_1_grams'] = number_of_character_1_grams(df.loc[i]['post_text'])\n",
    "    df.at[i, 'number_of_character_2_grams'] = number_of_character_2_grams(df.loc[i]['post_text'])\n",
    "    df.at[i, 'number_of_character_3_grams'] = number_of_character_3_grams(df.loc[i]['post_text'])\n",
    "    df.at[i, 'clindex'] = clindex(df.loc[i]['post_text'])\n",
    "    df.at[i, 'formality_measure'] = formality_measure(df.loc[i]['post_text'])\n",
    "    df.at[i, 'is_exclamation_question_mark_present'] = is_exclamation_question_mark_present(df.loc[i]['post_text'])\n",
    "    df.at[i, 'lix'] = lix(df.loc[i]['post_text'])\n",
    "    df.at[i, 'number_of_uppercase_words'] = number_of_uppercase_words(df.loc[i]['post_text'])\n",
    "    df.at[i, 'number_of_words'] = number_of_words(df.loc[i]['post_text'])\n",
    "    df.at[i, 'rix'] = rix(df.loc[i]['post_text'])\n",
    "    df.at[i, 'number_of_word_1_grams'] = number_of_word_1_grams(df.loc[i]['post_text'])\n",
    "# print(df)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize features\n",
    "\n",
    "* TODO get features in range of [0,1] or [-1,1]?\n",
    "* TODO convert lix to five levels (0-4) based on this: very easy (0-24), easy (25-34), standard (35-44), difficult (45-54) and very difficult (more than 55)\n",
    "* TODO convert rix to thirteen levels (0-13) based on this: 0.2, 0.5, 0.8, 1.3, 1.8, 2.4, 3.0, 3.7, 4.5, 5.3, 6.2, 7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Before removing features\n",
    "\n",
    "- RandomForest\n",
    "  - F-measure\n",
    "    - 0.776\n",
    "  - ROC Area\n",
    "    - 0.761\n",
    "\n",
    "\n",
    "- MultilayerPerceptron\n",
    "  - F-measure\n",
    "    - 0.781\n",
    "  - ROC Area\n",
    "    - 0.773\n",
    "\n",
    "## After removing features\n",
    "\n",
    "Removed features based on Weka's CfsSubsetEval Attribute Evaluator (-P 1 -E 1) and BestFirst Search method (-D 1 -N 5).\n",
    "\n",
    "- Kept features\n",
    "  - number_of_words\n",
    "  - length of the longest word\n",
    "  - whether start with number\n",
    "  - whether start with who/what/why/where/when/how\n",
    "  - number_of_character_1_grams\n",
    "  - clindex\n",
    "  - formality_measure\n",
    "  - is_exclamation_question_mark_present\n",
    "  - number_of_uppercase_words\n",
    "  - rix\n",
    "\n",
    "\n",
    "- RandomForest\n",
    "  - F-measure\n",
    "    - 0.779\n",
    "  - ROC Area\n",
    "    - 0.760\n",
    "    \n",
    "\n",
    "- MultilayerPerceptron\n",
    "  - F-measure\n",
    "    - 0.775\n",
    "  - ROC Area\n",
    "    - 0.774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
